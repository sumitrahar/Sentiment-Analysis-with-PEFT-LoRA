# Sentiment-Analysis-with-PEFT-LoRA
Efficient fine-tuning of DistilBERT for sentiment analysis using Parameter-Efficient Fine-Tuning (PEFT) with LoRA (Low-Rank Adaptation).

ğŸ“Œ Overview
This project demonstrates how to perform efficient sentiment analysis fine-tuning using:

ğŸ¤— Hugging Face Transformers

Parameter-Efficient Fine-Tuning (PEFT)

LoRA (Low-Rank Adaptation) technique

Google Colab-friendly implementation

Achieves good performance while using 98% fewer trainable parameters compared to full fine-tuning.

âœ¨ Key Features
ğŸš€ Efficient Training: ~5MB adapter weights vs 440MB full model

ğŸ’» Hardware Friendly: Works on both CPU and GPU
ğŸ”„ Flexible Adaptation: Easy to modify for custom datasets

ğŸ“Š Interpretable Results: Direct sentiment labels (POSITIVE/NEGATIVE)

ğŸ§  Knowledge Preservation: Maintains original model capabilities

ğŸ› ï¸ Technical Stack
Component	Technology Used
Base Model	distilbert-base-uncased
Fine-tuning Method	LoRA (PEFT)
Framework	PyTorch + Hugging Face
Hardware Requirements	CPU/GPU (4GB+ VRAM for GPU)
Key Libraries	Transformers, Datasets, PEFT
